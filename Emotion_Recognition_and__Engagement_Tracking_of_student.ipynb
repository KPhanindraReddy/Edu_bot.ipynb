{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyONpLVOrDnLsTVuBcxf05YB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KPhanindraReddy/PB4_ai_classroom_assistant_Intel_Internship/blob/main/Emotion_Recognition_and__Engagement_Tracking_of_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem statement 4:AI-Powered Interactive Learning Assistant for Classrooms"
      ],
      "metadata": {
        "id": "0AiJ5_q5PgXd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Emotion Recognition and Engagement Tracking of student**\n",
        "\n",
        "**Restart the runtime:**\n",
        "* Runtime → Restart runtime\n",
        "* Then click Runtime → Run all again\n",
        "* After giving camera permission again, the webcam should start working properly\n",
        "\n",
        "**Camera Permissions**\n",
        "* When prompted, allow access to your webcam in the browser\n",
        "* If the webcam feed does appear slowly, it is likely due to a glitch in Colab\n"
      ],
      "metadata": {
        "id": "HI29y0BlPnPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install openvino-dev[onnx]\n",
        "!pip install mediapipe\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import time\n",
        "from openvino.runtime import Core\n",
        "from IPython.display import display, HTML\n",
        "from base64 import b64decode, b64encode\n",
        "import os\n",
        "from google.colab.output import eval_js\n",
        "\n",
        "\n",
        "if not os.path.exists('model'):\n",
        "    os.makedirs('model')\n",
        "\n",
        "!omz_downloader --name face-detection-adas-0001 --precisions FP32 -o model\n",
        "!omz_downloader --name facial-landmarks-35-adas-0002 --precisions FP32 -o model\n",
        "!omz_downloader --name emotions-recognition-retail-0003 --precisions FP32 -o model\n",
        "\n",
        "\n",
        "ie = Core()\n",
        "\n",
        "\n",
        "print(\"Loading OpenVINO models...\")\n",
        "face_model = ie.read_model(model='model/intel/face-detection-adas-0001/FP32/face-detection-adas-0001.xml')\n",
        "face_compiled_model = ie.compile_model(model=face_model, device_name=\"CPU\")\n",
        "face_input_layer = face_compiled_model.input(0)\n",
        "_, _, H, W = face_input_layer.shape\n",
        "print(f\"Face detection input shape: {H}x{W}\")\n",
        "\n",
        "landmark_model = ie.read_model(model='model/intel/facial-landmarks-35-adas-0002/FP32/facial-landmarks-35-adas-0002.xml')\n",
        "landmark_compiled_model = ie.compile_model(model=landmark_model, device_name=\"CPU\")\n",
        "landmark_input_layer = landmark_compiled_model.input(0)\n",
        "_, _, LANDMARK_H, LANDMARK_W = landmark_input_layer.shape\n",
        "print(f\"Landmark model input shape: {LANDMARK_H}x{LANDMARK_W}\")\n",
        "\n",
        "emotion_model = ie.read_model(model='model/intel/emotions-recognition-retail-0003/FP32/emotions-recognition-retail-0003.xml')\n",
        "emotion_compiled_model = ie.compile_model(model=emotion_model, device_name=\"CPU\")\n",
        "emotion_input_layer = emotion_compiled_model.input(0)\n",
        "_, _, EMOTION_H, EMOTION_W = emotion_input_layer.shape\n",
        "print(f\"Emotion model input shape: {EMOTION_H}x{EMOTION_W}\")\n",
        "\n",
        "emotion_labels = ['neutral', 'happy', 'sad', 'surprise', 'anger']\n",
        "\n",
        "print(\"Models loaded successfully!\")\n",
        "\n",
        "display(HTML('''\n",
        "<div style=\"display: flex; flex-direction: column; align-items: center;\">\n",
        "    <div>\n",
        "        <video id=\"video\" width=\"640\" height=\"480\" autoplay style=\"display:none;\"></video>\n",
        "        <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n",
        "        <img id=\"outputImage\" width=\"640\" height=\"480\" style=\"border: 2px solid #4CAF50; border-radius: 5px;\">\n",
        "    </div>\n",
        "    <div style=\"margin-top: 20px;\">\n",
        "        <button id=\"stopButton\" style=\"padding:10px;font-size:16px;margin:10px;background:#f44336;color:white;border:none;border-radius:5px;\">\n",
        "            Stop Monitoring\n",
        "        </button>\n",
        "        <div id=\"status\" style=\"padding:10px;font-family:Arial;font-size:16px;\">Initializing camera...</div>\n",
        "    </div>\n",
        "</div>\n",
        "<script>\n",
        "\n",
        "window.monitoringActive = true;\n",
        "const video = document.getElementById('video');\n",
        "const canvas = document.getElementById('canvas');\n",
        "const outputImage = document.getElementById('outputImage');\n",
        "const stopButton = document.getElementById('stopButton');\n",
        "const statusDiv = document.getElementById('status');\n",
        "\n",
        "function captureFrame() {\n",
        "    if (video.videoWidth === 0 || video.videoHeight === 0) {\n",
        "        return '';\n",
        "    }\n",
        "    canvas.width = video.videoWidth;\n",
        "    canvas.height = video.videoHeight;\n",
        "    const ctx = canvas.getContext('2d');\n",
        "    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);\n",
        "    return canvas.toDataURL('image/jpeg', 0.8);\n",
        "}\n",
        "\n",
        "\n",
        "function updateOutputImage(dataUrl) {\n",
        "    outputImage.src = dataUrl;\n",
        "}\n",
        "\n",
        "\n",
        "function isMonitoringActive() {\n",
        "    return window.monitoringActive;\n",
        "}\n",
        "\n",
        "\n",
        "async function setupCamera() {\n",
        "    try {\n",
        "        const stream = await navigator.mediaDevices.getUserMedia({\n",
        "            video: { width: 640, height: 480, facingMode: 'user' }\n",
        "        });\n",
        "        video.srcObject = stream;\n",
        "        statusDiv.textContent = 'Camera ready. Monitoring active...';\n",
        "        return true;\n",
        "    } catch (err) {\n",
        "        statusDiv.textContent = 'Camera error: ' + err.message;\n",
        "        return false;\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "stopButton.addEventListener('click', function() {\n",
        "    window.monitoringActive = false;\n",
        "    if (video.srcObject) {\n",
        "        video.srcObject.getTracks().forEach(track => track.stop());\n",
        "    }\n",
        "    statusDiv.textContent = 'Monitoring stopped.';\n",
        "    stopButton.textContent = 'Stopped';\n",
        "    stopButton.disabled = true;\n",
        "    stopButton.style.background = '#9e9e9e';\n",
        "});\n",
        "\n",
        "\n",
        "setupCamera();\n",
        "</script>\n",
        "'''))\n",
        "\n",
        "\n",
        "state_colors = {\n",
        "    'engaged': (0, 255, 0),          # Green - Engaged\n",
        "    'disengaged': (0, 0, 255),        # Red - Disengaged\n",
        "    'looking_down': (255, 255, 0),    # Yellow - Looking down\n",
        "    'unknown': (128, 128, 128)        # Gray - Unknown\n",
        "}\n",
        "\n",
        "print(\"Starting student engagement monitoring with OpenVINO...\")\n",
        "print(\"Click 'Stop' button to exit\")\n",
        "\n",
        "def get_frame():\n",
        "    \"\"\"Capture frame using predefined JavaScript function\"\"\"\n",
        "    return eval_js(\"captureFrame()\")\n",
        "\n",
        "def update_output(data_url):\n",
        "    \"\"\"Update output image in browser\"\"\"\n",
        "    display(HTML(f\"<script>updateOutputImage('{data_url}');</script>\"))\n",
        "\n",
        "\n",
        "\n",
        "def detect_faces_ov(frame):\n",
        "\n",
        "    resized = cv2.resize(frame, (W, H))\n",
        "    input_image = np.expand_dims(resized.transpose(2, 0, 1), axis=0)\n",
        "\n",
        "\n",
        "    results = face_compiled_model([input_image])[face_compiled_model.output(0)]\n",
        "    faces = []\n",
        "\n",
        "\n",
        "    for detection in results[0][0]:\n",
        "        confidence = detection[2]\n",
        "        if confidence > 0.3:\n",
        "            x_min = int(detection[3] * frame.shape[1])\n",
        "            y_min = int(detection[4] * frame.shape[0])\n",
        "            x_max = int(detection[5] * frame.shape[1])\n",
        "            y_max = int(detection[6] * frame.shape[0])\n",
        "            w = x_max - x_min\n",
        "            h = y_max - y_min\n",
        "\n",
        "            if w > 30 and h > 30:\n",
        "                faces.append((x_min, y_min, w, h))\n",
        "\n",
        "    return faces\n",
        "\n",
        "\n",
        "def detect_landmarks_ov(face_roi):\n",
        "\n",
        "    resized = cv2.resize(face_roi, (LANDMARK_W, LANDMARK_H))\n",
        "    input_image = np.expand_dims(resized.transpose(2, 0, 1), axis=0).astype(np.float32)\n",
        "\n",
        "\n",
        "    landmarks = landmark_compiled_model([input_image])[landmark_compiled_model.output(0)]\n",
        "    return landmarks[0].reshape(-1, 2)\n",
        "\n",
        "\n",
        "\n",
        "def detect_emotion_ov(face_roi):\n",
        "\n",
        "    resized = cv2.resize(face_roi, (EMOTION_W, EMOTION_H))\n",
        "    input_image = np.expand_dims(resized.transpose(2, 0, 1), axis=0).astype(np.float32)\n",
        "\n",
        "\n",
        "    emotions = emotion_compiled_model([input_image])[emotion_compiled_model.output(0)]\n",
        "    emotion_idx = np.argmax(emotions)\n",
        "    confidence = emotions[0][emotion_idx]\n",
        "\n",
        "    return emotion_labels[emotion_idx], confidence\n",
        "\n",
        "\n",
        "def determine_state(face_position, frame_width):\n",
        "    x, y, w, h = face_position\n",
        "\n",
        "\n",
        "    face_center_x = x + w//2\n",
        "    frame_center_x = frame_width // 2\n",
        "    position_ratio = (face_center_x - frame_center_x) / frame_center_x\n",
        "\n",
        "\n",
        "    if abs(position_ratio) > 0.3:\n",
        "        return 'disengaged'\n",
        "\n",
        "    return 'engaged'\n",
        "\n",
        "\n",
        "frame_count = 0\n",
        "start_time = time.time()\n",
        "student_counts = []\n",
        "emotion_counts = {e: 0 for e in emotion_labels}\n",
        "state_counts = {s: 0 for s in state_colors}\n",
        "\n",
        "try:\n",
        "    print(\"Initializing camera...\")\n",
        "    time.sleep(3)\n",
        "\n",
        "    while frame_count < 300:\n",
        "        frame_count += 1\n",
        "\n",
        "\n",
        "        if not eval_js('isMonitoringActive()'):\n",
        "            print(\"Monitoring stopped by user\")\n",
        "            break\n",
        "\n",
        "\n",
        "        js_reply = get_frame()\n",
        "        if not js_reply or ',' not in js_reply:\n",
        "            continue\n",
        "\n",
        "\n",
        "        _, data = js_reply.split(',', 1)\n",
        "        nparr = np.frombuffer(b64decode(data), np.uint8)\n",
        "        frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "\n",
        "        if frame is None or frame.size == 0:\n",
        "            continue\n",
        "\n",
        "\n",
        "        frame = cv2.flip(frame, 1)\n",
        "        annotated_frame = frame.copy()\n",
        "        height, width = frame.shape[:2]\n",
        "\n",
        "\n",
        "        faces = detect_faces_ov(frame)\n",
        "        student_count = len(faces)\n",
        "        student_counts.append(student_count)\n",
        "\n",
        "\n",
        "        for (x, y, w, h) in faces:\n",
        "\n",
        "            face_roi = frame[y:y+h, x:x+w]\n",
        "            if face_roi.size == 0:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "\n",
        "                emotion, confidence = detect_emotion_ov(face_roi)\n",
        "                emotion_counts[emotion] += 1\n",
        "\n",
        "\n",
        "                state = determine_state((x, y, w, h), width)\n",
        "                state_counts[state] = state_counts.get(state, 0) + 1\n",
        "\n",
        "\n",
        "                color = state_colors.get(state, (128, 128, 128))\n",
        "                cv2.rectangle(annotated_frame, (x, y), (x+w, y+h), color, 3)\n",
        "\n",
        "\n",
        "                label = f\"{state.capitalize()} | {emotion.capitalize()}\"\n",
        "                text_y = y - 10 if y > 30 else y + 30\n",
        "                cv2.putText(annotated_frame, label, (x, text_y),\n",
        "                           cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
        "\n",
        "                cv2.putText(annotated_frame, f\"{confidence*100:.1f}%\",\n",
        "                           (x, text_y + 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (200, 200, 0), 2)\n",
        "\n",
        "            except Exception as e:\n",
        "\n",
        "                continue\n",
        "\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "        fps = frame_count / elapsed_time if elapsed_time > 0 else 0\n",
        "        cv2.putText(annotated_frame, f\"FPS: {fps:.1f}\", (10, 30),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "        cv2.putText(annotated_frame, f\"Students: {student_count}\", (10, 70),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "        cv2.putText(annotated_frame, f\"Frame: {frame_count}/300\", (10, 110),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "\n",
        "        status_text = \"Monitoring Active\" if eval_js('isMonitoringActive()') else \"Monitoring Stopped\"\n",
        "        status_color = (0, 255, 0) if eval_js('isMonitoringActive()') else (0, 0, 255)\n",
        "        cv2.putText(annotated_frame, status_text, (width - 250, 30),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, status_color, 2)\n",
        "\n",
        "\n",
        "        _, buffer = cv2.imencode('.jpg', annotated_frame)\n",
        "        data_url = f\"data:image/jpeg;base64,{b64encode(buffer).decode()}\"\n",
        "        update_output(data_url)\n",
        "\n",
        "\n",
        "        if frame_count % 10 == 0:\n",
        "            print(f\"Frame {frame_count}: Faces: {student_count}\")\n",
        "\n",
        "        time.sleep(0.01)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {str(e)}\")\n",
        "    import traceback\n",
        "    print(traceback.format_exc())\n",
        "\n",
        "\n",
        "if frame_count > 0:\n",
        "    avg_students = np.mean(student_counts) if student_counts else 0\n",
        "    elapsed_time = time.time() - start_time\n",
        "    avg_fps = frame_count / elapsed_time if elapsed_time > 0 else 0\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"CLASSROOM MONITORING REPORT\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Monitoring duration: {elapsed_time:.1f} seconds\")\n",
        "    print(f\"Total frames processed: {frame_count}\")\n",
        "    print(f\"Average FPS: {avg_fps:.1f}\")\n",
        "    print(f\"Average students detected: {avg_students:.1f}\")\n",
        "\n",
        "    print(\"\\nATTENTION STATES DISTRIBUTION:\")\n",
        "    total_states = sum(state_counts.values())\n",
        "    for state, count in state_counts.items():\n",
        "        percentage = (count / total_states) * 100 if total_states > 0 else 0\n",
        "        print(f\"- {state.upper()}: {count} frames ({percentage:.1f}%)\")\n",
        "\n",
        "    print(\"\\nEMOTION DISTRIBUTION:\")\n",
        "    total_emotions = sum(emotion_counts.values())\n",
        "    for emotion, count in emotion_counts.items():\n",
        "        percentage = (count / total_emotions) * 100 if total_emotions > 0 else 0\n",
        "        print(f\"- {emotion.upper()}: {count} frames ({percentage:.1f}%)\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"MONITORING COMPLETE\")\n",
        "    print(\"=\"*50)\n",
        "else:\n",
        "    print(\"No frames processed. Please check camera access and try again.\")"
      ],
      "metadata": {
        "id": "9rYFvBO9j7-W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}